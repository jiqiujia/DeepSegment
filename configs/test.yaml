# Data
data: 'data/test/'
max_time_step: 150
shared_vocab: False
unk: True
label_smoothing: 0.1
scale: 1 # Proportion of the training set
refF: ''


# Logging
logdir: 'experiments/'
report_interval: 1
eval_interval: 1
save_interval: 1
metrics: ['mle']


# Optimization
num_epoch: 1000000
optim: 'adam'
learning_rate: 1
learning_rate_decay: 0.95
start_decay_steps: 10000
decay_method: "noam"
beta1: 0.9
beta2: 0.998
max_grad_norm: 1
warmup_steps: 1000
epoch_decay: False # decay by epochs after decay starts
schedule: False # Learning rate schedule
schesamp: False # Scheduled sampling


# Model
model: "transformer_crf"
embed_only: False
positional: True
heads: 4
d_ff: 1024
convolutional: False
residual: True
attention: 'luong_gate'
param_init: 0
param_init_glorot: True
embedding_dim: 512
hidden_dim: 512
num_layers: 4
bidirectional: True
enc_num_layers: 4
dec_num_layers: 2
dropout: 0.1
emb_dropout: 0.1
swish: False
length_norm: True
pool_size: 0 # Pool size of maxout layer


# Others
seed: 1234

## KOBE
conditioned: False
knowledge: False
